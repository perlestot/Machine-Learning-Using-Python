{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn to compute basic features such as number of words, number of characters, average word length and number of special characters (such as Twitter hashtags and mentions). You will also learn to compute readability scores and determine the amount of education required to comprehend a piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format for ML algorithms\n",
    "- Excellent! This dataframe has numerical training features and the predictor variable is a class. Therefore, it is in a suitable format for applying a classification algorithm."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('../datasets/ted.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding\n",
    "\n",
    "In the previous exercise, we encountered a dataframe df1 which contained categorical features and therefore, was unsuitable for applying ML algorithms to.\n",
    "\n",
    "In this exercise, your task is to convert df1 into a format that is suitable for machine learning."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great job! You have successfully performed one-hot encoding on this dataframe. Notice how the feature 5 (which represents sex) gets converted to two features feature 5_male and feature 5_female. With one-hot encoding performed, df1 only contains numerical features and can now be fed into any standard ML model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic feature extraction\n",
    "\n",
    "## Character count of Russian tweets\n",
    "\n",
    "In this exercise, you have been given a dataframe tweets which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight.\n",
    "\n",
    "Your task is to create a new feature 'char_count' in tweets which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the content feature of tweets.\n",
    "\n",
    "Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.462\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('../datasets/russian_tweets.csv')\n",
    "\n",
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Notice that the average character count of these tweets is approximately 104, which is much higher than the overall average tweet length of around 40 characters. Depending on what you're working on, this may be something worth investigating into. For your information, there is research that indicates that fake news articles tend to have longer titles! Therefore, even extremely basic features such as character counts can prove to be very useful in certain applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count of TED talks\n",
    "\n",
    "ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the transcript feature in ted.\n",
    "\n",
    "In order to complete this task, you will need to define a function count_words that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the transcript feature of ted to create the new feature word_count and compute its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987.1\n"
     ]
    }
   ],
   "source": [
    "ted = pd.read_csv('../datasets/ted.csv')\n",
    "\n",
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing work! You now know how to compute the number of words in a given piece of text. Also, notice that the average length of a talk is close to 2000 words. You can use the word_count feature to compute its correlation with other variables such as number of views, number of comments, etc. and derive extremely interesting insights about TED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags and mentions in Russian tweets\n",
    "\n",
    "Let's revisit the tweets dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions count_hashtags() and count_mentions() respectively and applying them to the content feature of tweets.\n",
    "\n",
    "In case you don't recall, the tweets are contained in the content feature of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You now have a good grasp of how to compute various types of summary features. In the next lesson, we will learn about more advanced features that are capable of capturing more nuanced information beyond simple word and character counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability tests\n",
    "## Readability of 'The Myth of Sisyphus'\n",
    "\n",
    "In this exercise, you will compute the Flesch reading ease score for Albert Camus' famous essay The Myth of Sisyphus. We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay.\n",
    "\n",
    "The entire essay is in the form of a string and is available as sisyphus_essay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flesch Reading Ease is 81.67\n"
     ]
    }
   ],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "file = open('../datasets/sisyphus_essay.txt','r', encoding ='utf-8')\n",
    "sisyphus_essay = file.read()\n",
    "\n",
    "# Compute the readability scores\n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! You now know to compute the Flesch reading ease score for a given body of text. Notice that the score for this essay is approximately 81.67. This indicates that the essay is at the readability level of a 6th grade American student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability of various publications\n",
    "\n",
    "In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog index and consequently, determine the relative difficulty of reading these publications.\n",
    "\n",
    "The excerpts are available as the following strings:\n",
    "\n",
    "- `forbes`- An excerpt from an article from Forbes magazine on the Chinese social credit score system.\n",
    "- `harvard_law`- An excerpt from a book review published in Harvard Law Review.\n",
    "- `r_digest`- An excerpt from a Reader's Digest article on flight turbulence.\n",
    "- `time_kids` - An excerpt from an article on the ill effects of salt consumption published in TIME for Kids."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You are now adept at computing readability scores for various pieces of text. Notice that the Harvard Law Review excerpt has the highest Gunning fog index; indicating that it can be comprehended only by readers who have graduated college. On the other hand, the Time for Kids article, intended for children, has a much lower fog index and can be comprehended by 5th grade students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Lemmatization\n",
    "\n",
    "## Tokenizing the Gettysburg Address\n",
    "\n",
    "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
    "\n",
    "The entire speech is available as a string named gettysburg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "file = open('../datasets/gettysburg.txt','r', encoding ='utf-8')\n",
    "gettysburg = file.read()\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You now know how to tokenize a piece of text. In the next exercise, we will perform similar steps and conduct lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing the Gettysburg address\n",
    "\n",
    "In this exercise, we will perform lemmatization on the same gettysburg address from before.\n",
    "\n",
    "However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceived and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation might live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicate to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! You're now proficient at performing lemmatization using spaCy. Observe the lemmatized version of the speech. It isn't very readable to humans but it is in a much more convenient format for a machine to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "\n",
    "## Cleaning a blog post\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "Exercise\n",
    "Cleaning a blog post\n",
    "\n",
    "In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.\n",
    "\n",
    "The excerpt is available as a string blog and has been printed to the console. The list of stopwords are available as stopwords.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Using list comprehension, loop through doc to extract the lemma_ of each token.\n",
    "Remove stopwords and non-alphabetic tokens using stopwords and isalpha().\n",
    "\n",
    "Take Hint (-30 XP)\n",
    "\n",
    " +100 XP\n",
    "Great job! Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinge way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Five Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "file = open('../datasets/blog.txt','r', encoding ='utf-8')\n",
    "blog = file.read()\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = STOP_WORDS\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning TED talks in a dataframe\n",
    "\n",
    "In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe ted consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function preprocess and applying it to the transcript feature of the dataframe.\n",
    "\n",
    "The stopwords list is available as stopwords.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Generate the Doc object for text. Ignore the disable argument for now.\n",
    "Generate lemmas using list comprehension using the lemma_ attribute.\n",
    "Remove non-alphabetic characters using isalpha() in the if condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent job! You have preprocessed all the TED talk transcripts contained in ted and it is now in a good shape to perform operations such as vectorization (as we will soon see how). You now have a good understanding of how text preprocessing works and why it is important. In the next lessons, we will move on to generating word level features for our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building tf-idf document vectors\n",
    "\n",
    "## tf-idf weight of commonly occurring words\n",
    "The word bottle occurs 5 times in a particular document D and also occurs in every document of the corpus. What is the tf-idf weight of bottle in D?\n",
    "> Correct! In fact, the tf-idf weight for bottle in every document will be 0. This is because the inverse document frequency is constant across documents in a corpus and since bottle occurs in every document, its value is log(1), which is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf vectors for TED talks\n",
    "\n",
    "In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
    "\n",
    "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript.\n",
    "- Import TfidfVectorizer from sklearn.\n",
    "- Create a TfidfVectorizer object. Name it vectorizer.\n",
    "- Generate tfidf_matrix for ted using the fit_transform() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('../datasets/ted.csv')\n",
    "\n",
    "ted = df1.transcript\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29158)\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! You now know how to generate tf-idf vectors for a given corpus of text. You can use these vectors to perform predictive modeling just like we did with CountVectorizer. In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "- Cosine Similarity is one of the most popular metrics in NLP.\n",
    "- It measures the how far away two vectors are from one another: \n",
    "\n",
    "![](https://github.com/Shoklan/datacamp/raw/master/NLPFeatureEngineeringPython/images/Cosine-Similarity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range of cosine scores\n",
    "Which of the following is a possible cosine score for a pair of document vectors?\n",
    "> Great job! Since document vectors use only non-negative weights, the cosine score lies between 0 and 1.\n",
    "\n",
    "## Computing dot product\n",
    "\n",
    "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays.\n",
    "\n",
    "- Initialize A (1,3) and B (-2,2) as numpy arrays using np.array().\n",
    "- Compute the dot product using np.dot() and passing A and B as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Initialize numpy vectors\n",
    "A = np.array([1,3])\n",
    "B = np.array([-2,2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! The dot product of the two vectors is 1 * -2 + 3 * 2 = 4, which is indeed the output produced. We will not be using np.dot() too much in this course but it can prove to be a helpful function while computing dot products between two standalone vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity matrix of a corpus\n",
    "\n",
    "In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
    "\n",
    "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "- Initialize an instance of TfidfVectorizer. Name it tfidf_vectorizer.\n",
    "- Using fit_transform(), generate the tf-idf vectors for corpus. Name it tfidf_matrix.\n",
    "- Use cosine_similarity() and pass tfidf_matrix to compute the cosine similarity matrix cosine_sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = ['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a plot line based recommender\n",
    "\n",
    "## Comparing linear_kernel and cosine_similarity\n",
    "\n",
    "In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n",
    "\n",
    "We will then compare the computation times for both functions.\n",
    "1. Compute the cosine similarity matrix for tfidf_matrix using cosine_similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#  Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.01328730583190918 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the cosine similarity matrix for tfidf_matrix using linear_kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.004376649856567383 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it's because the dataset we're dealing with is extremely small and Python's time module is incapable of capture such minute time differences accurately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot recommendation engine\n",
    "\n",
    "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you.\n",
    "\n",
    "You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
    "\n",
    "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises.\n",
    "\n",
    "- Initialize a TfidfVectorizer with English stop_words. Name it tfidf.\n",
    "- Construct tfidf_matrix by fitting and transforming the movie plot data using fit_transform().\n",
    "- Generate the cosine similarity matrix cosine_sim using tfidf_matrix. Don't use cosine_similarity()!\n",
    "- Use get_recommendations() to generate recommendations for 'The Dark Knight Rises'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data be as close as Datacamp given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title       0\n",
       "overview    0\n",
       "tagline     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('../datasets/movie_overviews.csv')\n",
    "# df2 = df2.dropna(subset=['overview'])\n",
    "df2 = df2.dropna(subset=['tagline'])\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>949</td>\n",
       "      <td>Heat</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>A Los Angeles Crime Saga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>390734</td>\n",
       "      <td>Kingsglaive: Final Fantasy XV</td>\n",
       "      <td>The magical kingdom of Lucis is home to the wo...</td>\n",
       "      <td>Kingsglaive: Final Fantasy XV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9093</th>\n",
       "      <td>390989</td>\n",
       "      <td>Sharknado 4: The 4th Awakens</td>\n",
       "      <td>The new installment of the Sharknado franchise...</td>\n",
       "      <td>What happens in Vegas, stays in Vegas. Unless ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9095</th>\n",
       "      <td>392572</td>\n",
       "      <td>Rustom</td>\n",
       "      <td>Rustom Pavri, an honourable officer of the Ind...</td>\n",
       "      <td>Decorated Officer. Devoted Family Man. Defendi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>315011</td>\n",
       "      <td>Shin Godzilla</td>\n",
       "      <td>From the mind behind Evangelion comes a hit la...</td>\n",
       "      <td>A god incarnate. A city doomed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9098</th>\n",
       "      <td>391698</td>\n",
       "      <td>The Beatles: Eight Days a Week - The Touring Y...</td>\n",
       "      <td>The band stormed Europe in 1963, and, in 1964,...</td>\n",
       "      <td>The band you know. The story you don't.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7033 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "1       8844                                            Jumanji   \n",
       "2      15602                                   Grumpier Old Men   \n",
       "3      31357                                  Waiting to Exhale   \n",
       "4      11862                        Father of the Bride Part II   \n",
       "5        949                                               Heat   \n",
       "...      ...                                                ...   \n",
       "9091  390734                      Kingsglaive: Final Fantasy XV   \n",
       "9093  390989                       Sharknado 4: The 4th Awakens   \n",
       "9095  392572                                             Rustom   \n",
       "9097  315011                                      Shin Godzilla   \n",
       "9098  391698  The Beatles: Eight Days a Week - The Touring Y...   \n",
       "\n",
       "                                               overview  \\\n",
       "1     When siblings Judy and Peter discover an encha...   \n",
       "2     A family wedding reignites the ancient feud be...   \n",
       "3     Cheated on, mistreated and stepped on, the wom...   \n",
       "4     Just when George Banks has recovered from his ...   \n",
       "5     Obsessive master thief, Neil McCauley leads a ...   \n",
       "...                                                 ...   \n",
       "9091  The magical kingdom of Lucis is home to the wo...   \n",
       "9093  The new installment of the Sharknado franchise...   \n",
       "9095  Rustom Pavri, an honourable officer of the Ind...   \n",
       "9097  From the mind behind Evangelion comes a hit la...   \n",
       "9098  The band stormed Europe in 1963, and, in 1964,...   \n",
       "\n",
       "                                                tagline  \n",
       "1             Roll the dice and unleash the excitement!  \n",
       "2     Still Yelling. Still Fighting. Still Ready for...  \n",
       "3     Friends are the people who let you be yourself...  \n",
       "4     Just When His World Is Back To Normal... He's ...  \n",
       "5                              A Los Angeles Crime Saga  \n",
       "...                                                 ...  \n",
       "9091                      Kingsglaive: Final Fantasy XV  \n",
       "9093  What happens in Vegas, stays in Vegas. Unless ...  \n",
       "9095  Decorated Officer. Devoted Family Man. Defendi...  \n",
       "9097                    A god incarnate. A city doomed.  \n",
       "9098            The band you know. The story you don't.  \n",
       "\n",
       "[7033 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "Jumanji                                                  0\n",
       "Grumpier Old Men                                         1\n",
       "Waiting to Exhale                                        2\n",
       "Father of the Bride Part II                              3\n",
       "Heat                                                     4\n",
       "                                                      ... \n",
       "Kingsglaive: Final Fantasy XV                         7028\n",
       "Sharknado 4: The 4th Awakens                          7029\n",
       "Rustom                                                7030\n",
       "Shin Godzilla                                         7031\n",
       "The Beatles: Eight Days a Week - The Touring Years    7032\n",
       "Name: new_id, Length: 7033, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index\n",
    "# movie_plots = df2.overview\n",
    "movie_plots = df2.tagline\n",
    "metadata = df2\n",
    "\n",
    "df_temp = df2.copy()\n",
    "# df_temp.index.names = ['new_id']\n",
    "df_temp = df_temp.reset_index(drop=True)\n",
    "df_temp = df_temp.reset_index()\n",
    "df_temp = df_temp.rename(columns={'index': 'new_id'})\n",
    "df_temp = df_temp[['title' ,'new_id']]\n",
    "df_temp = df_temp.set_index('title')\n",
    "indices = df_temp['new_id']\n",
    "indices\n",
    "# df_temp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "movie_plots = df2.overview\n",
    "movie_plots.dropna\n",
    "indices = pd.Series(df2[['title' ,'id']].values)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7827    Harry Potter and the Deathly Hallows: Part 2\n",
      "8305                           The Hangover Part III\n",
      "8749                                         Taken 3\n",
      "1662                                       Tall Tale\n",
      "3056                                     Coyote Ugly\n",
      "8618                                        Hercules\n",
      "8266                                          Stoker\n",
      "1374                                       Dark City\n",
      "1594                                  The Black Hole\n",
      "7072                                           Saw V\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've just built your very first recommendation system. Notice how the recommender correctly identifies 'The Dark Knight Rises' as a Batman movie and recommends other Batman movies as a result. This sytem is, of course, very primitive and there are a host of ways in which it could be improved. One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations. We will not be covering this in this course but you have all the tools necessary to accomplish this. Do give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The recommender function\n",
    "\n",
    "In this exercise, we will build a recommender function get_recommendations(), as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
    "\n",
    "You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console.\n",
    "\n",
    "- Get index of the movie that matches the title by using the title key of indices.\n",
    "- Extract the ten most similar movies from sim_scores and store it back in sim_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! With this recommender function in our toolkit, we are now in a very good place to build the rest of the components of our recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TED talk recommender\n",
    "\n",
    "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you.\n",
    "\n",
    "You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
    "\n",
    "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce.\n",
    "\n",
    "- Initialize a TfidfVectorizer with English stopwords. Name it tfidf.\n",
    "- Construct tfidf_matrix by fitting and transforming transcripts.\n",
    "- Generate the cosine similarity matrix cosine_sim using tfidf_matrix.\n",
    "- Use get_recommendations() to generate recommendations for '5 ways to kill your dreams'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You have successfully built a TED talk recommender. This recommender works surprisingly well despite being trained only on a small subset of TED talks. In fact, three of the talks recommended by our system is also recommended by the official TED website as talks to watch next after '5 ways to kill your dreams'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond n-grams: word embeddings\n",
    "## Generating word vectors\n",
    "\n",
    "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'I like apples and oranges'\n",
    "\n",
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.19372894\n",
      "I apples 0.0821518\n",
      "I and -0.013632407\n",
      "I oranges 0.049489032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like I 0.19372894\n",
      "like like 1.0\n",
      "like apples 0.25841203\n",
      "like and 0.14297311\n",
      "like oranges 0.12997378\n",
      "apples I 0.0821518\n",
      "apples like 0.25841203\n",
      "apples apples 1.0\n",
      "apples and -0.04025854\n",
      "apples oranges 0.61655116\n",
      "and I -0.013632407\n",
      "and like 0.14297311\n",
      "and apples -0.04025854\n",
      "and and 1.0\n",
      "and oranges -0.076360986\n",
      "oranges I 0.049489032\n",
      "oranges like 0.12997378\n",
      "oranges apples 0.61655116\n",
      "oranges and -0.076360986\n",
      "oranges oranges 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/Users/PLoTAir/opt/anaconda3/envs/DS_project/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
    "\n",
    "Important note\n",
    "\n",
    "To make them compact and fast, spaCy’s small models (all packages that end in sm) don’t ship with word vectors, and only include context-sensitive tensors. This means you can still use the similarity() methods to compare documents, spans and tokens – but the result won’t be as good, and individual tokens won’t have any vectors assigned. So in order to use real word vectors, you need to download a larger model:\n",
    "\n",
    "- python -m spacy download en_core_web_sm\n",
    "+ python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Notice how the words 'apples' and 'oranges' have the highest pairwaise similarity score. This is expected as they are both fruits and are more related to each other than any other pair of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing similarity of Pink Floyd songs\n",
    "\n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as hopes, hey and mother respectively.\n",
    "\n",
    "Your task is to compute the pairwise similarity between mother and hopes, and mother and hey.\n",
    "\n",
    "- Create Doc objects for mother, hopes and hey.\n",
    "- Compute the similarity between mother and hopes.\n",
    "- Compute the similarity between mother and hey."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Notice that 'Mother' and 'Hey You' have a similarity score of 0.9 whereas 'Mother' and 'High Hopes' has a score of only 0.6. This is probably because 'Mother' and 'Hey You' were both songs from the same album 'The Wall' and were penned by Roger Waters. On the other hand, 'High Hopes' was a part of the album 'Division Bell' with lyrics by David Gilmour and his wife, Penny Samson. Treat yourself by listening to these songs. They're some of the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
